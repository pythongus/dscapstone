---
title: "Capstone Project - Describing the SwiftKey Challenge"
author: "Gus Garcia"
date: "30 September 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", warning=FALSE)
```

## Abstract

The capstone project for the Data Science Specialization is a predictive algorithm similar to that developed by SwiftKey, which powers Android smart phones. This algorithm can suggest the next word or words after the user has typed the first one.

This document explains major features of the data identified in the English dataset provided as a input data for the app, and briefly summarizes the plans for creating the prediction algorithm and Shiny app for a technical and non-technical audience. 

### Questions To Consider
* How can you efficiently store an n-gram model (think Markov Chains)?
* How can you use the knowledge about word frequencies to make your model smaller and more efficient?
* How many parameters do you need (i.e. how big is n in your n-gram model)?
* Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
* How do you evaluate whether your model is any good?
* How can you use backoff models to estimate the probability of unobserved n-grams?

## Initial Exploratory Data Analysis (EDA)

The dataset for this initial EDA is the English dataset. For other languages, there is some degree of uncertainty around the fitness of the algorithm without any modification, due to linguistic differences. This should be considered in the future stages of the app. Files included are:

English           German            Finnish           Russian 
----------------- ----------------- ----------------- ---------------
en_US.blogs.txt   de_DE.blogs.txt   fi_FI.blogs.txt   ru_RU.blogs.txt
en_US.news.txt    de_DE.news.txt    fi_FI.news.txt    ru_RU.news.txt
en_US.twitter.txt de_DE.twitter.txt fi_FI.twitter.txt ru_RU.twitter.txt

```{r datasets, echo=F}
dataFolder <- "final/en_US"
datasets <- list()
datasets$languages <- c("en_US", "de_DE", "fi_FI", "ru_RU")
datasets$sources <- c("blogs", "news", "twitter")

dt <- matrix(nrow = 3, ncol = 1)
for (lang in datasets$languages) {
    col <- c()
    for (src in datasets$sources) {
        col <- c(col, paste(lang, src, 'txt', sep = '.'))
    }
    dt <- cbind(dt, col)
}
dt <- dt[1:3, 2:5]
```

The main goal of this report is to understand the data and display some basic understading - and possible insights - towards the creation of a prediction algorithm that can be used in the finall data product, namely, a web app hosted on [shinyapp.io](https://shinyapp.io), otherwise know as ShinyApp.

### Counting the Number of Lines in Each Dataset

The initial statistic from the files is the number of lines on each of them, along with the number of words. This introduces the magnitude of the dataset and provides information towards space and memory allocation. The app should be able to predict sequences of words in a timely manner (less than 1 second).

The functions in script `dataset_tools.R` below optimizes the reading of the files. It reads the file by chuncks, avoiding memory congestion.
```{r}
source("dataset_tools.R")
```


`checkLines` is a function to assist with the specific tasks for this assingment. It receives a filename, a search function, an initial value, and the number of records to read. It searches the file in filename using the search function, reading nrec records at a time. The initial value is an accumlator for the search process, with a list of values.

### Reading the datasets and counting the number of lines and words

The results are shown in the table below using the function `counter`
```{r echo=F}
library(stringr)
```

```{r count_lines, comment=""}
counter <- function(lines, result) {
    nwords <- sum(unlist(str_split(lines, " ")) != "")
    nlines <- length(lines)
    list(lines=result$lines + nlines, words=result$words + nwords)
}
```

#### Plots for lines and words

```{r echo=F, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)
```

```{r plot_info_function}
plot_info <- function(dataset, total) {
    pl <- plot_total(dataset, total$lines, "Line")
    pw <- plot_total(dataset, total$words, "Word")
    grid.arrange(pl, pw, nrow = 1)
}

```

```{r english, cache=TRUE}
plot_info(dt[,1], read_datasets(dt[,1], counter))
```


```{r german, cache=TRUE}
plot_info(dt[,2], read_datasets(dt[,2], counter))
```

```{r finnish, cache=TRUE}
plot_info(dt[,3], read_datasets(dt[,3], counter))
```

```{r russian, cache=TRUE}
plot_info(dt[,4], read_datasets(dt[,4], counter))
```


## Creating N-Grams

The n-grams will be used to feed the prediction algorithm in the next stages for known and unknown sequences of words. The basic statistics for 2-gram and 3-gram from the four languages are shown below.

```{r fetch_ngrams, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)

fetch_ngrams <- function(lines, result) {
    tb <- data_frame(txt = lines)
    df <- tb %>%
          unnest_tokens(ngram, txt, token = "ngrams", n = result$n) %>%
          arrange(ngram) %>%
          group_by(ngram) %>%
          count
    if (is.null(result$ngrams)) {
        result$ngrams <- df
    } else {
        merged <- merge(result$ngrams, df, by = "ngram", all = "TRUE")
        merged[is.na(merged)] <- 0
        result$ngrams <- merged %>%
                         select(ngram, n.x, n.y) %>%
                         mutate(n = n.x + n.y) %>%
                         select(ngram, n)
    }
    result
}
```

```{r}
create_ngrams <- function(filenames, ngram_type=2) {
    for (file in filenames) {
        rdata = paste(file, "RData", sep = ".")
        if (!file.exists(rdata)) {
            ngrams <- checkLines(file,
                                  fetch_ngrams,
                                  initial_value=list(ngrams=NULL, n=ngram_type),
                                  nrec=100000)
            df <- ngrams$ngrams %>%
                  group_by(ngram) %>%
                  summarise(n = sum(n)) %>%
                  arrange(desc(n))
            save(df, file = rdata)
        }
        break
    }
}

create_ngrams(dt, 3)
```
