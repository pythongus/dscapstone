---
title: "Capstone Project - NLP - Week 1"
author: "Gus Garcia"
date: "29 August 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", warning=FALSE)
```

## SwiftKey And NLP

This is the capstone project from Johns Hopkins' Data Science Specialization. The project consists of using NLP to analyse text from different data sources.

## Initial Questions To Consider

* What do the data look like?
* Where do the data come from?
* Can you think of any other data sources that might help you in this project?
* What are the common steps in natural language processing?
* What are some common issues in the analysis of text data?
* What is the relationship between NLP and the concepts you have learned in the Specialization?

## Fetching And Reading The Initial Dataset

The data source used is in `en_US/en_US.blogs.txt`. Reading the first 5 lines of the file
```{r data_fetch, comment=""}

readFile <- function(filename, numLines=5, con=NULL, rootFolder="final/en_US/") {
    if (is.null(con))
        con <- file(paste0(rootFolder, filename), "r")
    lines <- readLines(con, numLines)
    list(connection=con, lines=lines)
}

ret <- readFile("en_US.blogs.txt")
close(ret$connection)
ret$lines
```

A function to assist with the next assignments. It receives a filename, a search function, an initial value, and the number of records to read. It searches the file in filename using the search function, reading nrec records at a time. The initial value is an accumlator for the search process, with a list of values.
```{r}

checkLines <- function(filename, search_function, initial_value=NULL, nrec=1000) {
    ret <- readFile(filename, nrec)
    result <- search_function(ret$lines, initial_value)
    while (length(ret$lines) == nrec) {
        ret <- readFile(filename, nrec, ret$connection)
        result <- search_function(ret$lines, result)
    }
    close(ret$connection)
    result
}
```

Reading the Twitter data source and counting the number of lines
```{r twitter, comment=""}

countLines <- function(lines, result) {
    list(counter=result$counter + length(lines))
}

checkLines("en_US.twitter.txt", countLines, list(counter=0))
```

Checking the longest line in all en_US files
```{r}
library(stringr)

lineLengths <- function(lines) {
    sapply(lines, function(line) {str_length(line)})
}

maxLineInFile <- function(filename) {
    
    function(lines, result) {
        currentmax <- result$length
        maxlen <- max(lineLengths(lines), currentmax)
        if (maxlen > currentmax) {
            list(length=maxlen, filename=filename)
        } else {
            result
        }
    }
}

files <- list.files("final/en_US/")
result <- list(length=0, filename="")
for (filename in files) {
    result <- checkLines(filename, maxLineInFile(filename), result)
}

result
```

Checking the occurrences of the words `love` and `hate` in the Twitter dataset
```{r love_and_hate, comment=""}

addLoveAndHate <- function(lines, result) {
    love <- sum(grepl(" love ", lines))
    hate <- sum(grepl(" hate ", lines))
    list(love=result$love + love, hate=result$hate + hate)
}

result <- checkLines("en_US.twitter.txt", addLoveAndHate, list(love=0, hate=0))
result$love / result$hate
```

Checking the tweet that match the word `biostats`
```{r biostats}

checkBiostats <- function(lines, result) {
    lineNumber <- grep(" biostats ", lines)
    if (length(lineNumber) > 0)
        list(line=lines[lineNumber])
    else
        result
}

checkLines("en_US.twitter.txt", checkBiostats, list(line=""))
```

Matching the sentence *A computer once beat me at chess, but it was no match for me at kickboxing*
```{r sentence}

sentence <- "A computer once beat me at chess, but it was no match for me at kickboxing"

checkSentence <- function(lines, result) {
    lineNumber <- grepl(sentence, lines)
    list(count=result$count + sum(lineNumber))
}

checkLines("en_US.twitter.txt", checkSentence, list(count=0))
```
