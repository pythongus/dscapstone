---
title: "Capstone Project - Describing the SwiftKey Challenge"
author: "Gus Garcia"
date: "30 September 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", warning=FALSE)
```

## Abstract

The capstone project for the Data Science Specialization is a predictive algorithm similar to that developed by SwiftKey, which powers Android smart phones. This algorithm can suggest the next word or words after the user has typed the first one.

This document explains major features of the data identified in the four language datasets provided as a input data for the app, and briefly summarizes the plans for creating the prediction algorithm and Shiny app for a technical and non-technical audience. 

### Questions To Consider
* How can you efficiently store an n-gram model (think Markov Chains)?
* How can you use the knowledge about word frequencies to make your model smaller and more efficient?
* How many parameters do you need (i.e. how big is n in your n-gram model)?
* Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
* How do you evaluate whether your model is any good?
* How can you use backoff models to estimate the probability of unobserved n-grams?

## Initial Exploratory Data Analysis (EDA)

The datasources for this initial EDA are the English, German, Finnish, and Russian datasets. For other languages, there is some degree of uncertainty around the fitness of the algorithm without any modification, due to linguistic differences. This should be considered in the future stages of the app. Files included are:

English           German            Finnish           Russian 
----------------- ----------------- ----------------- ---------------
en_US.blogs.txt   de_DE.blogs.txt   fi_FI.blogs.txt   ru_RU.blogs.txt
en_US.news.txt    de_DE.news.txt    fi_FI.news.txt    ru_RU.news.txt
en_US.twitter.txt de_DE.twitter.txt fi_FI.twitter.txt ru_RU.twitter.txt

```{r datasets, echo=F}
dataFolder <- "final"
datasets <- list()
datasets$languages <- c("en_US", "de_DE", "fi_FI", "ru_RU")
datasets$sources <- c("blogs", "news", "twitter")

dt <- matrix(nrow = 3, ncol = 1)
for (lang in datasets$languages) {
    col <- c()
    for (src in datasets$sources) {
        col <- c(col, paste(lang, src, 'txt', sep = '.'))
    }
    dt <- cbind(dt, col)
}
dt <- dt[1:3, 2:5]
```

The main goal of this report is to understand the data and display some basic understading - and possible insights - towards the creation of a prediction algorithm that can be used in the finall data product, namely, a web app hosted on [shinyapp.io](https://shinyapp.io), otherwise know as ShinyApp.

### Counting the Number of Lines in Each Dataset

The initial statistic from the files is the number of lines on each of them, along with the number of words. This introduces the magnitude of the dataset and provides information towards space and memory allocation. The app should be able to predict sequences of words in a timely manner (less than 1 second).

The functions in script `dataset_tools.R` below optimizes the reading of the files. It reads the file by chuncks, avoiding memory congestion.
```{r}
source("dataset_tools.R")
```


`checkLines` is a function to assist with the specific tasks for this assingment. It receives a filename, a search function, an initial value, and the number of records to read. It searches the file in filename using the search function, reading nrec records at a time. The initial value is an accumlator for the search process, with a list of values.

### Reading the datasets and counting the number of lines and words

The results are shown in the table below using the function `counter`
```{r echo=F}
library(stringr)
```

```{r count_lines, comment=""}
counter <- function(lines, result) {
    nwords <- sum(unlist(str_split(lines, " ")) != "")
    nlines <- length(lines)
    list(lines=result$lines + nlines, words=result$words + nwords)
}
```

#### Plots for lines and words

Some initial information from the datasets can be extracted from the word and line counts from each language.
```{r echo=F, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)
library(grid)
```

```{r plot_info_function}
plot_info <- function(dataset, total) {
    pl <- plot_total(dataset, total$lines, "Line")
    pw <- plot_total(dataset, total$words, "Word")
    grid.arrange(pl, pw, nrow = 1)
}

```

```{r english, cache=TRUE}
plot_info(dt[,1], read_datasets(dt[,1], counter))
```


```{r german, cache=TRUE}
plot_info(dt[,2], read_datasets(dt[,2], counter))
```

```{r finnish, cache=TRUE}
plot_info(dt[,3], read_datasets(dt[,3], counter))
```

```{r russian, cache=TRUE}
plot_info(dt[,4], read_datasets(dt[,4], counter))
```

### Some Observations

It is interesting to note that Finnish is quite different from the other languages, especially the use of the Twitter channel. Also, English is the language with greatest number of records, which can be taken into consideration when executing the code. Some sort of normalisation might be needed to make all performances similar, which will be discussed in the next section.

## Creating N-Grams

The n-grams will be used to feed the prediction algorithm in the next stages for known and unknown sequences of words. The basic statistics for 2-gram and 3-gram from the four languages are shown below. The strategy to read the datasets and extract the bigrams and trigrams is as follows:

* Read the next 50,000 records
* Extract the n-grams from the current set
* Group and count the occurrences
* Order in reverse order by the count
* Return the top 1000 records

The strategy is applicable to all datasets and the the goal is to optmise the reading regarding speed and memory, as well as extraction of meaninful information.

```{r fetch_ngrams, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)

fetch_ngrams <- function(lines, result) {
    records <- 1000
    tb <- data_frame(txt = lines)
    df <- tb %>%
          unnest_tokens(ngram, txt, token = "ngrams", n = result$n) %>%
          group_by(ngram) %>%
          count %>%
          arrange(desc(n)) %>%
          head(records)
    if (is.null(result$ngrams)) {
        result$ngrams <- df
    } else {
        merged <- merge(result$ngrams, df, by = "ngram", all = "TRUE")
        merged[is.na(merged)] <- 0
        result$ngrams <- merged %>%
                         select(ngram, n.x, n.y) %>%
                         mutate(n = n.x + n.y) %>%
                         select(ngram, n) %>%
                         arrange(desc(n)) %>%
                         head(records)
    }
    result
}
```

The below code takes care of creating the merged datasets, with the 1000 most frequest n-grams.

```{r}
create_ngrams <- function(filenames, ngram_type=2, nrec=50000) {
    for (file in filenames) {
        rdata = paste(file, "RData", sep = ".")
        if (!file.exists(rdata)) {
            ngrams <- checkLines(file,
                                 fetch_ngrams,
                                 initial_value=list(ngrams=NULL, n=ngram_type),
                                 nrec=nrec)
            df <- ngrams$ngrams %>%
                  group_by(ngram) %>%
                  summarise(n = sum(n)) %>%
                  arrange(desc(n))
            save(df, file = rdata)
        }
    }
}

create_and_move <- function(dataset, folder, ngram) {
    if (!dir.exists(folder)) {
        dir.create(folder)
        create_ngrams(dataset, ngram_type = ngram)
        for (file in dataset) {
            file.copy(from = file, to = folder)
            file.remove(file)
        }
    }
}

create_and_move(dt, "bigrams", 2)
create_and_move(dt, "trigrams", 3)
```

Once created, the files are stored in two differente directories, `bigrams` and `trigrams`, for later merging and further reduction.

### Merging The Datasets From The Four Languages

The code below will merge the datasets from the three different sources, after being transformed into n-grams datasets.
```{r dataset_merge}
library(rlist)

folders <- c("bigrams", "trigrams")
languages <- c("en", "de", "fi", "ru")
for (folder in folders) {
    ngram_files <- list.files(folder)
    for (language in languages) {
        files <- grep(paste0("^", language), ngram_files)
        merged <- NULL
        for (file in ngram_files[files]) {
            filename <- paste(folder, file, sep = .Platform$file.sep)
            load(filename)
            if (is.null(merged)) {
                merged <- df
            } else {
                merged <- merge(merged, df, by='ngram', all=TRUE)
                merged[is.na(merged)] <- 0
                merged <- merged %>%
                          mutate(n = n.x + n.y) %>%
                          arrange(desc(n)) %>%
                          select(ngram, n) %>%
                          head(1000)
            }
        }
        g <- ggplot(merged, aes(x='ngrams', y=n)) + geom_violin() +
             ggtitle(paste(folder, language, sep = " | "))
        print(g)
        print(summary(merged))
    }
}
```


## More Observations

The violin plots and the data summary show that the last quartile in all languages holde the most occurrences of each n-gram. This can be used for the final prediction model, to further optimise for speed for known 2-ngrams and 3-ngrams.

## GitHub Repository

[Code and transformed datasources](https://github.com/pythongus/dscapstone.git)